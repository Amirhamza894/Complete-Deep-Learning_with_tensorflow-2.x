{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedicated-marketing",
   "metadata": {},
   "source": [
    "# <mark>`Create a CNN model and Optimizing it using keras tuner`</mark>\n",
    "* Dataset = Fashion Mnist---> keras builtin dataset\n",
    "\n",
    "Kindly if you guys wants to run this notebook on CPU. \n",
    "\n",
    "##### ‚ùó kindly don't do this.. because this notebook takes arounds 2 hours to complete full iterations, trials and epochs\n",
    "\n",
    "##### üíØ Upload this notebook to google colab and change runtime to <mark>`GPU`</mark>... GPU gives us 4x-5x speed. 5 or 4 times faster than CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-hazard",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "verified-hardwood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fresh-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "moved-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "virtual-farmer",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[56]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-insurance",
   "metadata": {},
   "source": [
    "## <mark>`Data Normalization`</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sensitive-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lonely-offset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 28), (28, 28))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[2].shape, test_images[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "expired-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(len(train_images), 28, 28, 1)\n",
    "test_images = test_images.reshape(len(test_images), 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-turning",
   "metadata": {},
   "source": [
    "### <mark>`CNN Architecture`</mark>\n",
    "\n",
    "**Figure 1**\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/338199242/figure/fig1/AS:840889396113413@1577495063054/Basic-structure-of-a-CNN.jpg\">\n",
    "\n",
    "**Figure 2**\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/724/0*0zhQv-8SGGzVGETN.png\">\n",
    "\n",
    "**<mark>`filters`</mark>**\n",
    ">`filters` is multplying metrics with part of image to get edges of the images or parts of an images.\n",
    "\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRBPlW6IcEEhwhTkguVESbV_lEd26qF6lZn5Q&usqp=CAU\" width=\"500\" height=\"600\">\n",
    "\n",
    ">in the code below we specify filters that means selecting different values for filters\n",
    "`mix_value` and `max_values` for filters is how many filters we select between 32 and 128\n",
    "\n",
    "**<mark>`Kernel`</mark>**\n",
    ">`kernel` is the size of the filters `metrics values`.\n",
    "we specify values keras tuner will pick different values in that specific size as we mentioned\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://anhreynolds.com/img/cnn.png\">\n",
    "\n",
    "**<mark>`Flatten`</mark>**\n",
    "> `Flatten` just flat the values of metrics values of an image.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://missinglink.ai/wp-content/uploads/2019/04/Group-5.png\">\n",
    "\n",
    "**<mark>`Padding`</mark>**\\\n",
    "* same\n",
    "* valid\n",
    "\n",
    "<img src='https://aws1.discourse-cdn.com/business6/uploads/analyticsvidhya/original/2X/1/103763df32a7437e1ccefb15168856ae353dd597.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "suspended-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential([keras.layers.Conv2D(filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
    "                                                kernel_size = hp.Choice('conv_1_kernel', values = [3,5]),\n",
    "                                                activation='relu',\n",
    "                                                input_shape=(28,28,1)),\n",
    "                             keras.layers.Conv2D(filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n",
    "                                                kernel_size=hp.Choice('conv_2_kernel', values=[3,5]),\n",
    "                                                activation='relu'),\n",
    "                             keras.layers.Flatten(),\n",
    "                             keras.layers.Dense(units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),\n",
    "                                               activation = 'relu'),\n",
    "                             keras.layers.Dense(10, activation='softmax')])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-second",
   "metadata": {},
   "source": [
    "## <mark>`let's apply Keras tuner/hyperparametrs tuning`</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "accepted-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "informative-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_search = RandomSearch(build_model, objective='val_accuracy', max_trials=5,\n",
    "                            directory = 'output_directory', project_name = 'MNIST_fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-light",
   "metadata": {},
   "source": [
    "**this <mark>`search`</mark> function will search best parametrs for this build model**\n",
    "* remember when ever we give this <mark>`max_trials`</mark> it will run only for 2 epochs and with the help of those 2 epochs tuner will find best parameters.\n",
    "* latter on you can run any number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-anniversary",
   "metadata": {},
   "source": [
    "**<mark>`lets run to find the best hyperparameters for CNN model for fashion mnist dataset`</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "permanent-mechanism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 05m 02s]\n",
      "val_accuracy: 0.8558333516120911\n",
      "\n",
      "Best val_accuracy So Far: 0.9035000205039978\n",
      "Total elapsed time: 00h 33m 28s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_search.search(train_images, train_labels, epochs=3, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acoustic-davis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in output_directory\\MNIST_fashion\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_1_filter: 48\n",
      "conv_1_kernel: 5\n",
      "conv_2_filter: 48\n",
      "conv_2_kernel: 5\n",
      "dense_1_units: 112\n",
      "learning_rate: 0.001\n",
      "Score: 0.9035000205039978\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_1_filter: 128\n",
      "conv_1_kernel: 5\n",
      "conv_2_filter: 64\n",
      "conv_2_kernel: 5\n",
      "dense_1_units: 96\n",
      "learning_rate: 0.001\n",
      "Score: 0.9016666412353516\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_1_filter: 96\n",
      "conv_1_kernel: 3\n",
      "conv_2_filter: 48\n",
      "conv_2_kernel: 5\n",
      "dense_1_units: 48\n",
      "learning_rate: 0.01\n",
      "Score: 0.8730000257492065\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_1_filter: 64\n",
      "conv_1_kernel: 3\n",
      "conv_2_filter: 48\n",
      "conv_2_kernel: 3\n",
      "dense_1_units: 48\n",
      "learning_rate: 0.01\n",
      "Score: 0.8648333549499512\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_1_filter: 96\n",
      "conv_1_kernel: 5\n",
      "conv_2_filter: 64\n",
      "conv_2_kernel: 3\n",
      "dense_1_units: 64\n",
      "learning_rate: 0.01\n",
      "Score: 0.8558333516120911\n"
     ]
    }
   ],
   "source": [
    "tuner_search.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-borough",
   "metadata": {},
   "source": [
    "# ü§©ü§©ü§©\n",
    "### Ahaaaan\n",
    "you guys can see the best models is the first one which gives use 90+ accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "affected-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner_search.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "distinct-cursor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 48)        1248      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 48)        57648     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 112)               2150512   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1130      \n",
      "=================================================================\n",
      "Total params: 2,210,538\n",
      "Trainable params: 2,210,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-wages",
   "metadata": {},
   "source": [
    "**Now the Best model is selected and we can use this model on our training and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "conditional-updating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 109s 64ms/step - loss: 0.1616 - accuracy: 0.9386 - val_loss: 0.2801 - val_accuracy: 0.9063\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 107s 63ms/step - loss: 0.1226 - accuracy: 0.9536 - val_loss: 0.2784 - val_accuracy: 0.9152\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 103s 61ms/step - loss: 0.0942 - accuracy: 0.9641 - val_loss: 0.3150 - val_accuracy: 0.9123\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 101s 60ms/step - loss: 0.0707 - accuracy: 0.9725 - val_loss: 0.3553 - val_accuracy: 0.9148\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 100s 59ms/step - loss: 0.0561 - accuracy: 0.9789 - val_loss: 0.3629 - val_accuracy: 0.9157\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 103s 61ms/step - loss: 0.0452 - accuracy: 0.9839 - val_loss: 0.4451 - val_accuracy: 0.9105\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 101s 60ms/step - loss: 0.0382 - accuracy: 0.9858 - val_loss: 0.5209 - val_accuracy: 0.9130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27cc4bb5580>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10, validation_split=0.1, initial_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "concrete-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5523 - accuracy: 0.9066\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "composed-broadway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5523265600204468"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "talented-durham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9065999984741211"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-south",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
